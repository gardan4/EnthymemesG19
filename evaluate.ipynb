{
 "cells": [
  {
   "cell_type": "code",
   "id": "2fe7092497deeb29",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-21T17:44:21.724690Z",
     "start_time": "2024-11-21T17:44:01.860937Z"
    }
   },
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_texts(file_path):\n",
    "    \"\"\"\n",
    "    Load texts from a file and return as a list of stripped strings.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "def main():\n",
    "    # Define explicit pairs of hypothesis and target files\n",
    "    # Format: (hypothesis_file_path, target_file_path)\n",
    "    file_pairs = [\n",
    "        (\"generated_hypotheses/ikatdata_checkpoint16.hypo\", \"D3test/ikatdata.target\"),\n",
    "        (\"generated_hypotheses/ikatdata_checkpoint50.hypo\", \"D3test/ikatdata.target\"),\n",
    "        (\"generated_hypotheses/jandata_checkpoint16.hypo\", \"D2test/jandata.target\"),\n",
    "        (\"generated_hypotheses/jandata_checkpoint50.hypo\", \"D2test/jandata.target\"),\n",
    "        (\"generated_hypotheses/semevaldata_checkpoint16.hypo\", \"D1test/semevaldata.target\"),\n",
    "        (\"generated_hypotheses/semevaldata_checkpoint50.hypo\", \"D1test/semevaldata.target\")\n",
    "    ]\n",
    "    \n",
    "    # Output CSV file to store evaluation results\n",
    "    output_csv = 'evaluation_results.csv'\n",
    "    \n",
    "    # Initialize metrics\n",
    "    bleu = evaluate.load('bleu')\n",
    "    bertscore = evaluate.load('bertscore')\n",
    "    rouge = evaluate.load('rouge')  # Optional, if ROUGE is desired\n",
    "    \n",
    "    # Prepare CSV header\n",
    "    csv_header = ['Model', 'Source File', 'BLEU-1', 'BLEU-2', \n",
    "                  'ROUGE-1', 'ROUGE-2', 'ROUGE-L', \n",
    "                  'BERTScore Precision', 'BERTScore Recall', 'BERTScore F1']\n",
    "    \n",
    "    # Open CSV file for writing\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(csv_header)\n",
    "        \n",
    "        print(f\"Starting evaluation of {len(file_pairs)} file pairs.\\n\")\n",
    "        \n",
    "        # Iterate over each file pair with a progress bar\n",
    "        for hypo_path, target_path in tqdm(file_pairs, desc=\"Evaluating file pairs\"):\n",
    "            # Extract model identifier and source file name from the hypothesis file name\n",
    "            hypo_filename = os.path.basename(hypo_path)\n",
    "            try:\n",
    "                sourcefile, modelid = hypo_filename.split('_checkpoint')\n",
    "                modelid = f\"checkpoint{modelid.split('.hypo')[0]}\"\n",
    "            except Exception as e:\n",
    "                print(f\"Filename format error for {hypo_filename}: {e}\")\n",
    "                continue  # Skip this pair if filename is not as expected\n",
    "            \n",
    "            # Check if both files exist\n",
    "            if not os.path.exists(hypo_path):\n",
    "                print(f\"Hypothesis file not found: {hypo_path}. Skipping.\")\n",
    "                continue\n",
    "            if not os.path.exists(target_path):\n",
    "                print(f\"Target file not found: {target_path}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Load texts\n",
    "            generated_hypo_texts = load_texts(hypo_path)\n",
    "            target_texts = load_texts(target_path)\n",
    "            \n",
    "            # Validate that all datasets have the same number of samples\n",
    "            if len(target_texts) != len(generated_hypo_texts):\n",
    "                print(f\"Line count mismatch between hypothesis and target for {hypo_filename}. Skipping.\")\n",
    "                continue  # Skip if line counts do not match\n",
    "            \n",
    "            # Compute BLEU-1\n",
    "            bleu1 = bleu.compute(\n",
    "                predictions=generated_hypo_texts,\n",
    "                references=[[ref] for ref in target_texts],\n",
    "                max_order=1\n",
    "            )\n",
    "            \n",
    "            # Compute BLEU-2\n",
    "            bleu2 = bleu.compute(\n",
    "                predictions=generated_hypo_texts,\n",
    "                references=[[ref] for ref in target_texts],\n",
    "                max_order=2\n",
    "            )\n",
    "            \n",
    "            # Compute ROUGE\n",
    "            ro = rouge.compute(\n",
    "                predictions=generated_hypo_texts,\n",
    "                references=target_texts,\n",
    "                use_stemmer=True\n",
    "            )\n",
    "            \n",
    "            # Compute BERTScore\n",
    "            bs = bertscore.compute(\n",
    "                predictions=generated_hypo_texts,\n",
    "                references=target_texts,\n",
    "                lang='en',\n",
    "                model_type='bert-base-uncased',\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Calculate average BERTScore metrics\n",
    "            bert_precision = sum(bs['precision']) / len(bs['precision']) * 100\n",
    "            bert_recall = sum(bs['recall']) / len(bs['recall']) * 100\n",
    "            bert_f1 = sum(bs['f1']) / len(bs['f1']) * 100\n",
    "            \n",
    "            # Write results to CSV\n",
    "            writer.writerow([\n",
    "                modelid,\n",
    "                sourcefile,\n",
    "                f\"{bleu1['bleu'] * 100:.2f}\",\n",
    "                f\"{bleu2['bleu'] * 100:.2f}\",\n",
    "                f\"{ro['rouge1'] * 100:.2f}\",\n",
    "                f\"{ro['rouge2'] * 100:.2f}\",\n",
    "                f\"{ro['rougeL'] * 100:.2f}\",\n",
    "                f\"{bert_precision:.2f}\",\n",
    "                f\"{bert_recall:.2f}\",\n",
    "                f\"{bert_f1:.2f}\"\n",
    "            ])\n",
    "            \n",
    "            # Print summary for this pair\n",
    "            print(f\"Evaluated {hypo_filename}: BLEU-1={bleu1['bleu'] * 100:.2f}, BLEU-2={bleu2['bleu'] * 100:.2f}, ROUGE-1={ro['rouge1'] * 100:.2f}, ROUGE-2={ro['rouge2'] * 100:.2f}, ROUGE-L={ro['rougeL'] * 100:.2f}, BERTScore F1={bert_f1:.2f}\")\n",
    "        \n",
    "        print(f\"\\nEvaluation completed. Results saved to {output_csv}.\")\n",
    "\n",
    "# Execute the evaluation\n",
    "main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation of 6 file pairs.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating file pairs:  17%|█▋        | 1/6 [00:01<00:09,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated ikatdata_checkpoint16.hypo: BLEU-1=65.53, BLEU-2=58.81, ROUGE-1=66.05, ROUGE-2=53.27, ROUGE-L=63.53, BERTScore F1=75.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating file pairs:  33%|███▎      | 2/6 [00:02<00:05,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated ikatdata_checkpoint50.hypo: BLEU-1=61.09, BLEU-2=51.69, ROUGE-1=62.66, ROUGE-2=45.59, ROUGE-L=57.59, BERTScore F1=70.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating file pairs:  50%|█████     | 3/6 [00:03<00:03,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated jandata_checkpoint16.hypo: BLEU-1=10.74, BLEU-2=5.54, ROUGE-1=29.32, ROUGE-2=9.44, ROUGE-L=20.79, BERTScore F1=48.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating file pairs:  67%|██████▋   | 4/6 [00:04<00:02,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated jandata_checkpoint50.hypo: BLEU-1=8.52, BLEU-2=4.26, ROUGE-1=27.58, ROUGE-2=8.57, ROUGE-L=19.97, BERTScore F1=46.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating file pairs:  83%|████████▎ | 5/6 [00:11<00:02,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated semevaldata_checkpoint16.hypo: BLEU-1=57.01, BLEU-2=50.65, ROUGE-1=60.76, ROUGE-2=48.35, ROUGE-L=58.17, BERTScore F1=73.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating file pairs: 100%|██████████| 6/6 [00:17<00:00,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated semevaldata_checkpoint50.hypo: BLEU-1=61.20, BLEU-2=53.28, ROUGE-1=60.61, ROUGE-2=46.57, ROUGE-L=56.91, BERTScore F1=71.48\n",
      "\n",
      "Evaluation completed. Results saved to evaluation_results.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "77a8fef2631b0fbf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
