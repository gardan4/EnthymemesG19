{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe7092497deeb29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T22:37:49.798358Z",
     "start_time": "2024-11-21T22:37:46.502714Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation of 3 file pairs.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating file pairs:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target texts: ['And since Computer games do not originate from old traditions.']\n",
      "Hypothesis texts: ['And since She decided to go to the doctor. computer should also not be recognized as Olympic events.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating file pairs:  33%|███▎      | 1/3 [00:01<00:02,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated ikatdataparacomet_checkpoint16.hypo: BLEU-1=17.04, BLEU-2=9.64, ROUGE-1=26.13, ROUGE-2=9.75, ROUGE-L=20.94, BERTScore F1=52.27\n",
      "Target texts: ['[\\'\"He\" stands for Obama\\', \\'Getting troops out of countries impies that wars are ending.\\', \\'\"He\" stands for Obama.\\', \\'Wars are conducted with troops\\', \\'The war refers to Iraq.\\', \"With no troops there\\'s no war\", \\'Obama stated that he ended the war in Iraq.\\', \\'Obama pulled all US troops out of Iraq.\\']']\n",
      "Hypothesis texts: ['And since He decided to go to the store and had a war in the house. wars']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating file pairs:  67%|██████▋   | 2/3 [00:02<00:01,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated jandataparacomet_checkpoint16.hypo: BLEU-1=7.67, BLEU-2=3.50, ROUGE-1=23.75, ROUGE-2=5.97, ROUGE-L=16.64, BERTScore F1=43.98\n",
      "Target texts: [\"And since that helps the company's bottom line.\"]\n",
      "Hypothesis texts: ['And since He decided to go to the store and get a lot of money to get a lot of money to get a new job. Unpaid exploit college students students']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating file pairs: 100%|██████████| 3/3 [00:07<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated semevaldataparacomet_checkpoint16.hypo: BLEU-1=16.33, BLEU-2=8.58, ROUGE-1=22.45, ROUGE-2=8.12, ROUGE-L=18.90, BERTScore F1=49.12\n",
      "\n",
      "Evaluation completed. Results saved to evaluation_results.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_hypos(file_path, source_file_path):\n",
    "    \"\"\"\n",
    "    Load texts from a file and return as a list of stripped strings.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        #also read  source file to get the source text\n",
    "        with open(source_file_path, 'r', encoding='utf-8') as f_source:\n",
    "            line = [line.strip() for line in f.readlines()]\n",
    "            line_source = [line.strip() for line in f_source.readlines()]\n",
    "            #the source line contains a # , save the content before and after the # in two different vars\n",
    "            line_source = [line.split('#') for line in line_source]\n",
    "            #remove both elements of the list line_source from the line of the hypothesis\n",
    "            for i in range(len(line_source)):\n",
    "                line[i] = line[i].replace(line_source[i][0], '')\n",
    "                line[i] = line[i].replace(line_source[i][2], '')\n",
    "\n",
    "        return line\n",
    "    \n",
    "def load_targets(file_path, source_file_path):\n",
    "    \"\"\"\n",
    "    Load texts from a file and return as a list of stripped strings.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        #also read  source file to get the source text\n",
    "        with open(source_file_path, 'r', encoding='utf-8') as f_source:\n",
    "            line = [line.strip() for line in f.readlines()]\n",
    "            line_source = [line.strip() for line in f_source.readlines()]\n",
    "            #the source line contains a # , save the content before and after the # in two different vars\n",
    "            line_source = [line.split('#') for line in line_source]\n",
    "            #remove both elements of the list line_source from the line of the hypothesis\n",
    "            for i in range(len(line_source)):\n",
    "                line[i] = line[i].replace(line_source[i][0], '')\n",
    "                line[i] = line[i].replace(line_source[i][2], '')\n",
    "\n",
    "        return line\n",
    "\n",
    "def main():\n",
    "    # Define explicit pairs of hypothesis and target files\n",
    "    # Format: (hypothesis_file_path, target_file_path)\n",
    "    file_pairs = [\n",
    "        (\"generated_hypotheses/ikatdataparacomet_checkpoint16.hypo\", \"D3test/ikatdata.target\", \"D3test/ikatdataparacomet.source\"),\n",
    "        (\"generated_hypotheses/jandataparacomet_checkpoint16.hypo\", \"D2test/jandata.target\", \"D2test/jandataparacomet.source\"),\n",
    "        (\"generated_hypotheses/semevaldataparacomet_checkpoint16.hypo\", \"D1test/semevaldata.target\", \"D1test/semevaldataparacomet.source\"),\n",
    "    ]\n",
    "    \n",
    "    # Output CSV file to store evaluation results\n",
    "    output_csv = 'evaluation_results.csv'\n",
    "    \n",
    "    # Initialize metrics\n",
    "    bleu = evaluate.load('bleu')\n",
    "    bertscore = evaluate.load('bertscore')\n",
    "    rouge = evaluate.load('rouge')  # Optional, if ROUGE is desired\n",
    "    \n",
    "    # Prepare CSV header\n",
    "    csv_header = ['Model', 'Source File', 'BLEU-1', 'BLEU-2', \n",
    "                  'ROUGE-1', 'ROUGE-2', 'ROUGE-L', \n",
    "                  'BERTScore Precision', 'BERTScore Recall', 'BERTScore F1']\n",
    "    \n",
    "    # Open CSV file for writing\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(csv_header)\n",
    "        \n",
    "        print(f\"Starting evaluation of {len(file_pairs)} file pairs.\\n\")\n",
    "        \n",
    "        # Iterate over each file pair with a progress bar\n",
    "        for hypo_path, target_path, source_path in tqdm(file_pairs, desc=\"Evaluating file pairs\"):\n",
    "            # Extract model identifier and source file name from the hypothesis file name\n",
    "            hypo_filename = os.path.basename(hypo_path)\n",
    "            try:\n",
    "                sourcefile, modelid = hypo_filename.split('_checkpoint')\n",
    "                modelid = f\"checkpoint{modelid.split('.hypo')[0]}\"\n",
    "            except Exception as e:\n",
    "                print(f\"Filename format error for {hypo_filename}: {e}\")\n",
    "                continue  # Skip this pair if filename is not as expected\n",
    "            \n",
    "            # Check if both files exist\n",
    "            if not os.path.exists(hypo_path):\n",
    "                print(f\"Hypothesis file not found: {hypo_path}. Skipping.\")\n",
    "                continue\n",
    "            if not os.path.exists(target_path):\n",
    "                print(f\"Target file not found: {target_path}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Load texts\n",
    "            generated_hypo_texts = load_hypos(hypo_path, source_path)\n",
    "            target_texts = load_targets(target_path, source_path)\n",
    "            # print 5 first elements of the target and hypothesis texts\n",
    "            print(f\"Target texts: {target_texts[:1]}\")\n",
    "            print(f\"Hypothesis texts: {generated_hypo_texts[:1]}\")  \n",
    "\n",
    "            # Validate that all datasets have the same number of samples\n",
    "            if len(target_texts) != len(generated_hypo_texts):\n",
    "                print(f\"Line count mismatch between hypothesis and target for {hypo_filename}. Skipping.\")\n",
    "                continue  # Skip if line counts do not match\n",
    "            \n",
    "            # Compute BLEU-1\n",
    "            bleu1 = bleu.compute(\n",
    "                predictions=generated_hypo_texts,\n",
    "                references=[[ref] for ref in target_texts],\n",
    "                max_order=1\n",
    "            )\n",
    "            \n",
    "            # Compute BLEU-2\n",
    "            bleu2 = bleu.compute(\n",
    "                predictions=generated_hypo_texts,\n",
    "                references=[[ref] for ref in target_texts],\n",
    "                max_order=2\n",
    "            )\n",
    "            \n",
    "            # Compute ROUGE\n",
    "            ro = rouge.compute(\n",
    "                predictions=generated_hypo_texts,\n",
    "                references=target_texts,\n",
    "                use_stemmer=True\n",
    "            )\n",
    "            \n",
    "            # Compute BERTScore\n",
    "            bs = bertscore.compute(\n",
    "                predictions=generated_hypo_texts,\n",
    "                references=target_texts,\n",
    "                lang='en',\n",
    "                model_type='bert-base-uncased',\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Calculate average BERTScore metrics\n",
    "            bert_precision = sum(bs['precision']) / len(bs['precision']) * 100\n",
    "            bert_recall = sum(bs['recall']) / len(bs['recall']) * 100\n",
    "            bert_f1 = sum(bs['f1']) / len(bs['f1']) * 100\n",
    "            \n",
    "            # Write results to CSV\n",
    "            writer.writerow([\n",
    "                modelid,\n",
    "                sourcefile,\n",
    "                f\"{bleu1['bleu'] * 100:.2f}\",\n",
    "                f\"{bleu2['bleu'] * 100:.2f}\",\n",
    "                f\"{ro['rouge1'] * 100:.2f}\",\n",
    "                f\"{ro['rouge2'] * 100:.2f}\",\n",
    "                f\"{ro['rougeL'] * 100:.2f}\",\n",
    "                f\"{bert_precision:.2f}\",\n",
    "                f\"{bert_recall:.2f}\",\n",
    "                f\"{bert_f1:.2f}\"\n",
    "            ])\n",
    "            \n",
    "            # Print summary for this pair\n",
    "            print(f\"Evaluated {hypo_filename}: BLEU-1={bleu1['bleu'] * 100:.2f}, BLEU-2={bleu2['bleu'] * 100:.2f}, ROUGE-1={ro['rouge1'] * 100:.2f}, ROUGE-2={ro['rouge2'] * 100:.2f}, ROUGE-L={ro['rougeL'] * 100:.2f}, BERTScore F1={bert_f1:.2f}\")\n",
    "        \n",
    "        print(f\"\\nEvaluation completed. Results saved to {output_csv}.\")\n",
    "\n",
    "# Execute the evaluation\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a8fef2631b0fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
