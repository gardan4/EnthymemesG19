#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
evaluate_enthymemes.py

Reads .hypo files generated by generate_enthymemes.py and compares
each line's OUTPUT to a reference line from a target file.

Metrics:
- BLEU-1, BLEU-2 (using nltk)
- ROUGE-1, ROUGE-2, ROUGE-L (using huggingface evaluate "rouge")
- BERTScore Precision, Recall, F1 (using huggingface evaluate "bertscore")

Usage:
    python evaluate_enthymemes.py

Dependencies:
    pip install evaluate bert-score rouge-score nltk
"""

import os
import glob
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import evaluate

# Make sure NLTK data is available (e.g., 'punkt' tokenizer)
nltk.download('punkt', quiet=True)

###############################################################################
#                            CONFIGURATION                                    #
###############################################################################

# The reference file should have the same number of lines as the source, each line
# being the target enthymeme for the corresponding source line index.
REFERENCE_FILE = "Datasets/D1test/semevaldataparacomet.target"

# The folder or pattern where .hypo files are stored
# e.g., "O1-OPEN_OpenO1-LLama-8B-v0.1_zero_shot.hypo"
HYPO_GLOB_PATTERN = "*.hypo"

###############################################################################
#                            METRIC LOADERS                                   #
###############################################################################

# We'll use Hugging Face "evaluate" for ROUGE and BERTScore
rouge_evaluator = evaluate.load("rouge")
bertscore_evaluator = evaluate.load("bertscore")

###############################################################################
#                         EVALUATION FUNCTIONS                                #
###############################################################################

def compute_bleu_scores(system_lines, reference_lines):
    """
    Compute average BLEU-1 and BLEU-2 over all lines using nltk.translate.
    """
    smoothie = SmoothingFunction().method1

    bleu1_scores = []
    bleu2_scores = []

    for sys_line, ref_line in zip(system_lines, reference_lines):
        # Tokenize
        ref_tokens = nltk.word_tokenize(ref_line.lower())
        sys_tokens = nltk.word_tokenize(sys_line.lower())

        # BLEU-1 = weights=(1, 0, 0, 0)
        bleu1 = sentence_bleu(
            [ref_tokens],  # list of references
            sys_tokens,
            weights=(1.0, 0, 0, 0),
            smoothing_function=smoothie
        )
        bleu1_scores.append(bleu1)

        # BLEU-2 = weights=(0.5, 0.5, 0, 0)
        bleu2 = sentence_bleu(
            [ref_tokens],
            sys_tokens,
            weights=(0.5, 0.5, 0, 0),
            smoothing_function=smoothie
        )
        bleu2_scores.append(bleu2)

    avg_bleu1 = sum(bleu1_scores) / len(bleu1_scores) if bleu1_scores else 0.0
    avg_bleu2 = sum(bleu2_scores) / len(bleu2_scores) if bleu2_scores else 0.0

    return avg_bleu1, avg_bleu2


def compute_rouge_scores(system_lines, reference_lines):
    """
    Compute ROUGE scores (1, 2, L) using huggingface evaluate 'rouge'.
    Returns a dict with keys "rouge1", "rouge2", "rougeL".
    """
    results = rouge_evaluator.compute(
        predictions=system_lines,
        references=reference_lines
    )
    # The returned dict might have keys like "rouge1", "rouge2", "rougeL", each is a F1 measure
    # By default evaluate.load("rouge") includes F-measure for these. 
    # We'll just return them.
    return {
        "rouge1": results["rouge1"],
        "rouge2": results["rouge2"],
        "rougeL": results["rougeL"]
    }


def compute_bertscore(system_lines, reference_lines):
    """
    Compute BERTScore using huggingface evaluate 'bertscore'.
    Returns precision, recall, and f1 (averaged over all lines).
    """
    # By default, 'bertscore' uses roberta-large if not otherwise specified
    bscore_results = bertscore_evaluator.compute(
        predictions=system_lines,
        references=reference_lines,
        model_type="microsoft/deberta-xlarge-mnli",  # or "roberta-large", etc.
        lang="en"
    )
    # bscore_results has keys: 'precision', 'recall', 'f1', 'hash'
    # Each is a list of scores. We'll average them.
    precision = sum(bscore_results['precision']) / len(bscore_results['precision'])
    recall = sum(bscore_results['recall']) / len(bscore_results['recall'])
    f1 = sum(bscore_results['f1']) / len(bscore_results['f1'])
    return precision, recall, f1

###############################################################################
#                             MAIN EVAL SCRIPT                                #
###############################################################################

def main():
    # Make sure the reference file exists
    if not os.path.exists(REFERENCE_FILE):
        print(f"Reference file '{REFERENCE_FILE}' not found.")
        return

    # Read all reference lines
    with open(REFERENCE_FILE, 'r', encoding='utf-8') as f:
        reference_lines = [r.strip() for r in f if r.strip()]

    # Gather all .hypo files
    hypo_files = glob.glob(HYPO_GLOB_PATTERN)
    if not hypo_files:
        print("No .hypo files found to evaluate.")
        return

    print(f"Found {len(hypo_files)} .hypo files. Evaluating...\n")

    for hypo_file in hypo_files:
        # Read the system lines from the .hypo file
        # Each record is:
        #  INPUT (line_idx): ...
        #  OUTPUT:
        #  <system text>
        #  ================
        #
        # We'll read line_idx to match the reference line.
        # Then read the next line as "OUTPUT:", 
        # then the next line is the system text.

        system_lines = []
        with open(hypo_file, 'r', encoding='utf-8') as hf:
            lines = hf.readlines()

        line_idx = None
        next_is_output_text = False
        output_text_buffer = []

        for i, l in enumerate(lines):
            line_stripped = l.strip()
            if line_stripped.startswith("INPUT ("):
                # parse the line index
                # e.g. "INPUT (3): The hayride was in October..."
                # let's extract the integer inside parentheses
                try:
                    # "INPUT (3): ..." -> part between parentheses
                    start_paren = line_stripped.find("(")
                    end_paren = line_stripped.find(")")
                    if start_paren != -1 and end_paren != -1:
                        idx_str = line_stripped[start_paren+1:end_paren]
                        line_idx = int(idx_str)
                except:
                    line_idx = None

            elif line_stripped.startswith("OUTPUT:"):
                # the next line should be the actual system text
                next_is_output_text = True
                output_text_buffer = []
            elif line_stripped.startswith("="*20):
                # end of one record
                # store the joined system text (if any)
                if output_text_buffer and line_idx is not None:
                    sys_text = " ".join(output_text_buffer).strip()
                    # Ensure system_lines array has correct length
                    # We might have references for 0..N-1 lines
                    # We'll store in the same index as line_idx
                    while len(system_lines) <= line_idx:
                        system_lines.append("")
                    system_lines[line_idx] = sys_text
                line_idx = None
                next_is_output_text = False
                output_text_buffer = []
            else:
                # If next_is_output_text is True, we accumulate the lines
                if next_is_output_text:
                    output_text_buffer.append(line_stripped)

        # If the file ends without "====", we handle any leftover buffer
        if output_text_buffer and line_idx is not None:
            sys_text = " ".join(output_text_buffer).strip()
            while len(system_lines) <= line_idx:
                system_lines.append("")
            system_lines[line_idx] = sys_text

        # Now we have system_lines with possible empty entries if line_idx had gaps
        # Let's do a final trim if the system_lines array is larger than reference_lines
        if len(system_lines) > len(reference_lines):
            system_lines = system_lines[:len(reference_lines)]

        # If system_lines is shorter, fill the remainder with empty
        while len(system_lines) < len(reference_lines):
            system_lines.append("")

        # Compute metrics
        bleu1, bleu2 = compute_bleu_scores(system_lines, reference_lines)
        rouge_dict = compute_rouge_scores(system_lines, reference_lines)
        bert_p, bert_r, bert_f = compute_bertscore(system_lines, reference_lines)

        # Print summary
        print(f"File: {hypo_file}")
        print(f"  BLEU-1: {bleu1:.4f}")
        print(f"  BLEU-2: {bleu2:.4f}")
        print(f"  ROUGE-1 (F1): {rouge_dict['rouge1']:.4f}")
        print(f"  ROUGE-2 (F1): {rouge_dict['rouge2']:.4f}")
        print(f"  ROUGE-L (F1): {rouge_dict['rougeL']:.4f}")
        print(f"  BERTScore Precision: {bert_p:.4f}")
        print(f"  BERTScore Recall:    {bert_r:.4f}")
        print(f"  BERTScore F1:        {bert_f:.4f}")
        print("-"*60)

if __name__ == "__main__":
    main()
